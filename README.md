# Agentic-AI-DevSecOps-Framework

An intelligent DevSecOps framework powered by Google ADK agents for security analysis, test optimization, and CI/CD automation.

## Features

- ğŸ” **GNN-Powered Test Impact Prediction** - Predicts which tests are affected by code changes
- ğŸ§ª **Flaky Test Management** - Bayesian tracking, LLM log analysis, auto-quarantine
- ğŸ›¡ï¸ **Security Scanning Agents** - SAST, DAST, SCA, IAST analysis
- ğŸ“Š **Test Coverage Analysis** - Smart coverage recommendations
- ğŸš€ **CI/CD Integration** - GitHub Actions workflows

## Quick Start

```bash
# Clone and setup
cd Agentic-AI-devsecops-framework
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Set up environment
cp config/.env.example .env
# Edit .env with your API keys

# Run ADK web UI
cd agents/orchestrator
adk web
```

## Project Structure

```
â”œâ”€â”€ agents/                 # ADK Agent Packages
â”‚   â”œâ”€â”€ orchestrator/       # Root agent
â”‚   â”œâ”€â”€ gnn_agent/          # Test impact prediction
â”‚   â”œâ”€â”€ flaky_test_agent/   # Flaky test management
â”‚   â”œâ”€â”€ sast_agent/         # Static analysis
â”‚   â”œâ”€â”€ dast_agent/         # Dynamic analysis
â”‚   â””â”€â”€ sca_agent/          # Dependency scanning
â”œâ”€â”€ core/                   # Core components
â”œâ”€â”€ integrations/           # External tool integrations
â”œâ”€â”€ models/                 # ML models (GNN, Bayesian)
â”œâ”€â”€ evaluation/             # Thesis Evaluation Framework
â”‚   â”œâ”€â”€ datasets.py         # Benchmark definitions
â”‚   â”œâ”€â”€ metrics.py          # Thesis metrics (DDR, MTTD, BLEU)
â”‚   â””â”€â”€ runner.py           # Evaluation orchestrator
â”œâ”€â”€ dashboard/              # Developer Dashboard UI
â””â”€â”€ tests/                  # Test suite
```

## ğŸ“ Thesis Evaluation

This framework includes a comprehensive evaluation module for thesis research.

### Evaluating on GitHub Actions (Recommended)

1. Go to **Actions** â†’ **Thesis Evaluation Pipeline**
2. Click **Run workflow**
3. Select a benchmark dataset:
   - `webgoat`: OWASP WebGoat (Java)
   - `dvwa`: Damn Vulnerable Web App (PHP)
   - `juice-shop`: OWASP Juice Shop (JS)
   - `all`: Run all benchmarks
4. Download the artifacts for detailed JSON reports.

### Evaluating Locally

To evaluate on a specific dataset (e.g., WebGoat):

```bash
# Evaluate WebGoat
python -m evaluation.runner --dataset webgoat

# Evaluate all datasets with specific tools
python -m evaluation.runner --all --tools semgrep sast_agent
```

**Note**: Datasets are cloned to `evaluation/datasets/` and are excluded from git.

## Documentation

- [Implementation Plan](docs/implementation_plan.md)
- [API Reference](docs/api_reference.md)
- [Agent Guide](docs/agent_guide.md)

## License

MIT License
